#!/usr/bin/env python
# -- coding:utf-8 --

import torch
from torch import nn
import torch.nn.functional as F
import math

# Existing utility classes from your code
class ConvBNReLU(nn.Module):
    def __init__(self,
                 c_in,
                 c_out,
                 kernel_size,
                 stride=1,
                 padding=1,
                 activation=True):
        super(ConvBNReLU, self).__init__()
        self.conv = nn.Conv2d(c_in,
                              c_out,
                              kernel_size=kernel_size,
                              stride=stride,
                              padding=padding,
                              bias=False)
        self.bn = nn.BatchNorm2d(c_out)
        self.relu = nn.ReLU()
        self.activation = activation

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        if self.activation:
            x = self.relu(x)
        return x


class DoubleConv(nn.Module):
    def __init__(self, cin, cout):
        super(DoubleConv, self).__init__()
        self.conv = nn.Sequential(
            ConvBNReLU(cin, cout, 3, 1, padding=1),
            ConvBNReLU(cout, cout, 3, stride=1, padding=1, activation=False))
        self.conv1 = nn.Conv2d(cout, cout, 1)
        self.relu = nn.ReLU()
        self.bn = nn.BatchNorm2d(cout)

    def forward(self, x):
        x = self.conv(x)
        h = x
        x = self.conv1(x)
        x = self.bn(x)
        x = h + x
        x = self.relu(x)
        return x


# Transformer components
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class TransformerBlock(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = MultiHeadSelfAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
        self.norm2 = nn.LayerNorm(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_hidden_dim),
            nn.GELU(),
            nn.Dropout(drop),
            nn.Linear(mlp_hidden_dim, dim),
            nn.Dropout(drop)
        )

    def forward(self, x):
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x


class PatchEmbed(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x)
        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C
        return x


class TransformerEncoder(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12, num_heads=12):
        super().__init__()
        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        self.pos_embed = nn.Parameter(torch.zeros(1, (img_size // patch_size) ** 2, embed_dim))
        self.blocks = nn.ModuleList([
            TransformerBlock(dim=embed_dim, num_heads=num_heads)
            for _ in range(depth)])
        self.norm = nn.LayerNorm(embed_dim)
        
        # Initialize position embeddings
        self._init_weights()
        
    def _init_weights(self):
        # Initialize position embeddings
        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], 
                                             int(self.patch_embed.num_patches**0.5))
        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))
        
    def forward(self, x):
        # Input shape: B, C, H, W
        x = self.patch_embed(x)  # B, N, C
        x = x + self.pos_embed   # Add position embeddings
        
        # Transformer blocks
        features = []
        for i, block in enumerate(self.blocks):
            x = block(x)
            if i in [2, 5, 8, 11]:  # Collect features at different depths
                features.append(x)
        
        x = self.norm(x)
        features.append(x)
        
        return features


# Utility function for position embeddings
def get_2d_sincos_pos_embed(embed_dim, grid_size):
    """
    Create 2D sine-cosine positional embeddings.
    """
    # Create position indices
    grid_h = torch.arange(grid_size, dtype=torch.float32)
    grid_w = torch.arange(grid_size, dtype=torch.float32)
    grid = torch.meshgrid(grid_h, grid_w, indexing='ij')
    grid = torch.stack(grid, dim=0)
    
    # Reshape to 2x1xHxW format
    grid = grid.reshape(2, 1, grid_size, grid_size)
    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
    
    # Convert to numpy array before returning
    return pos_embed.numpy()

def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):
    """
    Convert grid to position embeddings.
    """
    assert embed_dim % 2 == 0
    
    # Use sine and cosine functions for position embeddings
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)
    
    emb = torch.cat([emb_h, emb_w], dim=1)  # (H*W, D)
    return emb


def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    Create 1D sine-cosine embeddings from positions.
    """
    assert embed_dim % 2 == 0
    omega = torch.arange(embed_dim // 2, dtype=torch.float32)
    omega /= embed_dim / 2.
    omega = 1. / 10000**omega  # (D/2,)
    
    pos = pos.reshape(-1)  # (M,)
    out = torch.einsum('m,d->md', pos, omega)  # (M, D/2)
    
    emb_sin = torch.sin(out)  # (M, D/2)
    emb_cos = torch.cos(out)  # (M, D/2)
    
    emb = torch.cat([emb_sin, emb_cos], dim=1)  # (M, D)
    return emb


# Feature Fusion Modules
class MemoryEfficientCrossAttention(nn.Module):
    def __init__(self, dim, chunk_size=1024):
        super().__init__()
        self.query_proj = nn.Linear(dim, dim)
        self.key_proj = nn.Linear(dim, dim)
        self.value_proj = nn.Linear(dim, dim)
        self.scale = dim ** -0.5
        self.chunk_size = chunk_size  # Process attention in chunks
        
    def forward(self, x, y):
        """Memory-efficient cross attention from x to y"""
        B, N, C = x.shape
        
        q = self.query_proj(x)  # B, N, C
        k = self.key_proj(y)    # B, M, C
        v = self.value_proj(y)  # B, M, C
        
        # Process attention in chunks to save memory
        out = torch.zeros_like(q)
        
        # Process in chunks along the sequence length dimension
        for i in range(0, N, self.chunk_size):
            end = min(i + self.chunk_size, N)
            # Compute attention scores for this chunk
            attn = torch.bmm(q[:, i:end], k.transpose(1, 2)) * self.scale  # B, chunk_size, M
            attn = F.softmax(attn, dim=-1)
            
            # Apply attention weights to values
            out[:, i:end] = torch.bmm(attn, v)  # B, chunk_size, C
            
        return out


class FeatureFusionModule(nn.Module):
    def __init__(self, cnn_dim, trans_dim):
        super().__init__()
        self.fusion_dim = cnn_dim
        
        # Project transformer features to CNN dimension
        self.trans_proj = nn.Linear(trans_dim, cnn_dim)
        
        # Use memory-efficient cross attention
        self.cross_attn = MemoryEfficientCrossAttention(cnn_dim)
        
        # Final fusion layers
        self.fusion = nn.Sequential(
            nn.Conv2d(cnn_dim * 2, cnn_dim, kernel_size=1),
            nn.BatchNorm2d(cnn_dim),
            nn.ReLU()
        )
        
    def forward(self, cnn_feat, trans_feat):
        B, C, H, W = cnn_feat.shape
        
        # Process transformer features
        trans_feat = self.trans_proj(trans_feat)  # B, N, C
        
        # Calculate proper reshape dimensions for transformer features
        trans_size = int(math.sqrt(trans_feat.shape[1]))
        
        # First downsample the CNN features if they're too large
        if H * W > 10000:  # Threshold to determine if downsampling is needed
            scale_factor = min(1.0, math.sqrt(10000 / (H * W)))
            target_h = max(16, int(H * scale_factor))
            target_w = max(16, int(W * scale_factor))
            cnn_feat_small = F.interpolate(cnn_feat, size=(target_h, target_w), mode='bilinear', align_corners=False)
            H, W = target_h, target_w
        else:
            cnn_feat_small = cnn_feat
        
        # Reshape transformer features to spatial form and resize
        trans_feat_spatial = trans_feat.reshape(B, trans_size, trans_size, C).permute(0, 3, 1, 2)
        trans_feat_resized = F.interpolate(trans_feat_spatial, size=(H, W), mode='bilinear', align_corners=False)
        
        # Flatten for attention
        cnn_feat_flat = cnn_feat_small.flatten(2).transpose(1, 2)  # B, H*W, C
        trans_feat_flat = trans_feat_resized.flatten(2).transpose(1, 2)  # B, H*W, C
        
        # Apply cross attention
        fused_feat = self.cross_attn(cnn_feat_flat, trans_feat_flat)
        fused_feat = fused_feat.transpose(1, 2).reshape(B, C, H, W)
        
        # Restore original resolution if downsampled
        if H != cnn_feat.shape[2] or W != cnn_feat.shape[3]:
            fused_feat = F.interpolate(fused_feat, size=(cnn_feat.shape[2], cnn_feat.shape[3]), 
                                      mode='bilinear', align_corners=False)
        
        # Concatenate and fuse
        out = torch.cat([cnn_feat, fused_feat], dim=1)
        out = self.fusion(out)
        
        return out


# Spatial Attention Module for refinement
class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        padding = kernel_size // 2
        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_pool = torch.mean(x, dim=1, keepdim=True)
        max_pool, _ = torch.max(x, dim=1, keepdim=True)
        pool = torch.cat([avg_pool, max_pool], dim=1)
        attn = self.conv(pool)
        attn = self.sigmoid(attn)
        return x * attn


# Hybrid UNet-Transformer Model
class HybridUNetTransformer(nn.Module):
    def __init__(self, out_ch=4, img_size=224):
        super(HybridUNetTransformer, self).__init__()
        
        # CNN Encoder (UNet encoder)
        self.enc1 = DoubleConv(3, 64)
        self.pool1 = nn.MaxPool2d(2)
        self.enc2 = DoubleConv(64, 128)
        self.pool2 = nn.MaxPool2d(2)
        self.enc3 = DoubleConv(128, 256)
        self.pool3 = nn.MaxPool2d(2)
        self.enc4 = DoubleConv(256, 512)
        self.pool4 = nn.MaxPool2d(2)
        
        # Transformer Encoder
        self.trans_encoder = TransformerEncoder(
            img_size=img_size,
            patch_size=16,
            in_chans=3,
            embed_dim=768,
            depth=12,
            num_heads=12
        )
        
        # Feature fusion modules
        self.fusion1 = FeatureFusionModule(64, 768)
        self.fusion2 = FeatureFusionModule(128, 768)
        self.fusion3 = FeatureFusionModule(256, 768)
        self.fusion4 = FeatureFusionModule(512, 768)
        
        # Bottleneck
        self.bottleneck = DoubleConv(512, 1024)
        
        # Spatial attention for bottleneck refinement
        self.spatial_attn = SpatialAttention(kernel_size=7)
        
        # Decoder
        self.up1 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec1 = DoubleConv(1024, 512)  # 512 (upsampled) + 512 (skip)
        self.up2 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec2 = DoubleConv(512, 256)   # 256 + 256
        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec3 = DoubleConv(256, 128)   # 128 + 128
        self.up4 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec4 = DoubleConv(128, 64)    # 64 + 64
        
        # Deep supervision outputs
        self.deep_sup1 = nn.Conv2d(512, out_ch, 1)
        self.deep_sup2 = nn.Conv2d(256, out_ch, 1)
        self.deep_sup3 = nn.Conv2d(128, out_ch, 1)
        
        # Output layer
        self.out = nn.Conv2d(64, out_ch, 1)
        
        # Initialize weights
        self._init_weights()
        
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
        
    def reshape_transformer_features(self, features, sizes):
        reshaped_features = []
        for feat, size in zip(features, sizes):
            B, N, C = feat.shape
            H = W = int(math.sqrt(N))
            feat = feat.transpose(1, 2).reshape(B, C, H, W)
            feat = F.interpolate(feat, size=size, mode='bilinear', align_corners=False)
            reshaped_features.append(feat)
        return reshaped_features
            
    def forward(self, x):
        # Handle single-channel input by repeating to 3 channels
        if x.size()[1] == 1:
            x = x.repeat(1, 3, 1, 1)
        
        # Get transformer features
        trans_features = self.trans_encoder(x)
        
        # CNN Encoder path
        e1 = self.enc1(x)       # (B, 64, 224, 224)
        p1 = self.pool1(e1)     # (B, 64, 112, 112)
        e2 = self.enc2(p1)      # (B, 128, 112, 112)
        p2 = self.pool2(e2)     # (B, 128, 56, 56)
        e3 = self.enc3(p2)      # (B, 256, 56, 56)
        p3 = self.pool3(e3)     # (B, 256, 28, 28)
        e4 = self.enc4(p3)      # (B, 512, 28, 28)
        p4 = self.pool4(e4)     # (B, 512, 14, 14)
        
        # Reshape transformer features to match CNN feature maps
        feature_sizes = [(224, 224), (112, 112), (56, 56), (28, 28), (14, 14)]
        reshaped_trans_features = self.reshape_transformer_features(trans_features, feature_sizes)
        
        # Feature fusion at multiple levels
        e1_fused = self.fusion1(e1, trans_features[0])
        e2_fused = self.fusion2(e2, trans_features[1])
        e3_fused = self.fusion3(e3, trans_features[2])
        e4_fused = self.fusion4(e4, trans_features[3])
        
        # Bottleneck
        b = self.bottleneck(p4)  # (B, 1024, 14, 14)
        
        # Apply spatial attention to refine bottleneck features
        b = self.spatial_attn(b)
        
        # Decoder with skip connections
        u1 = self.up1(b)       # (B, 512, 28, 28)
        c1 = torch.cat((u1, e4_fused), dim=1)  # (B, 1024, 28, 28)
        d1 = self.dec1(c1)     # (B, 512, 28, 28)
        ds1 = self.deep_sup1(d1)  # Deep supervision output 1
        
        u2 = self.up2(d1)      # (B, 256, 56, 56)
        c2 = torch.cat((u2, e3_fused), dim=1)  # (B, 512, 56, 56)
        d2 = self.dec2(c2)     # (B, 256, 56, 56)
        ds2 = self.deep_sup2(d2)  # Deep supervision output 2
        
        u3 = self.up3(d2)      # (B, 128, 112, 112)
        c3 = torch.cat((u3, e2_fused), dim=1)  # (B, 256, 112, 112)
        d3 = self.dec3(c3)     # (B, 128, 112, 112)
        ds3 = self.deep_sup3(d3)  # Deep supervision output 3
        
        u4 = self.up4(d3)      # (B, 64, 224, 224)
        c4 = torch.cat((u4, e1_fused), dim=1)  # (B, 128, 224, 224)
        d4 = self.dec4(c4)     # (B, 64, 224, 224)
        
        # Final output
        out = self.out(d4)     # (B, out_ch, 224, 224)
        
        # Upsample deep supervision outputs
        ds1 = F.interpolate(ds1, size=(224, 224), mode='bilinear', align_corners=False)
        ds2 = F.interpolate(ds2, size=(224, 224), mode='bilinear', align_corners=False)
        ds3 = F.interpolate(ds3, size=(224, 224), mode='bilinear', align_corners=False)
        
        if self.training:
            return out, ds1, ds2, ds3
        else:
            return out


# Training loss function
class DeepSupervisionLoss(nn.Module):
    def __init__(self, weights=[1.0, 0.4, 0.2, 0.1]):
        super(DeepSupervisionLoss, self).__init__()
        self.weights = weights
        self.dice_loss = DiceLoss()
        self.ce_loss = nn.CrossEntropyLoss()
        
    def forward(self, outputs, target):
        main_out = outputs[0]
        
        # Calculate main loss
        loss = self.weights[0] * (self.dice_loss(main_out, target) + self.ce_loss(main_out, target))
        
        # Calculate deep supervision losses
        for i in range(1, len(outputs)):
            loss += self.weights[i] * (self.dice_loss(outputs[i], target) + self.ce_loss(outputs[i], target))
            
        return loss


class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-5):
        super(DiceLoss, self).__init__()
        self.smooth = smooth
        
    def forward(self, pred, target):
        # Assume pred is logits: B, C, H, W
        # Convert target to one-hot: B, C, H, W
        pred = F.softmax(pred, dim=1)
        
        # Flatten the tensors
        pred = pred.view(-1)
        target = target.view(-1)
        
        intersection = (pred * target).sum()
        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)
        
        return 1 - dice


# Example usage
if __name__ == "__main__":
    model = HybridUNetTransformer(out_ch=4)
    x = torch.randn(1, 3, 224, 224)  # Example input
    y = model(x)
    print("Output shape:", y.shape)  # Should output: torch.Size([1, 4, 224, 224])
