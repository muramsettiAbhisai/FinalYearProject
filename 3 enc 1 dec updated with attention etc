#!/usr/bin/env python
# -*- coding:utf-8 -*-

import torch
from torch import nn
import torchvision.models as models
import torch.nn.functional as F

class ConvBNReLU(nn.Module):
    def __init__(self,
                 c_in,
                 c_out,
                 kernel_size,
                 stride=1,
                 padding=1,
                 activation=True):
        super(ConvBNReLU, self).__init__()
        self.conv = nn.Conv2d(c_in,
                              c_out,
                              kernel_size=kernel_size,
                              stride=stride,
                              padding=padding,
                              bias=False)
        self.bn = nn.BatchNorm2d(c_out)
        self.relu = nn.ReLU()
        self.activation = activation

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        if self.activation:
            x = self.relu(x)
        return x

class DoubleConv(nn.Module):
    def __init__(self, cin, cout):
        super(DoubleConv, self).__init__()
        self.conv = nn.Sequential(
            ConvBNReLU(cin, cout, 3, 1, padding=1),
            ConvBNReLU(cout, cout, 3, stride=1, padding=1, activation=False))
        self.conv1 = nn.Conv2d(cout, cout, 1)
        self.relu = nn.ReLU()
        self.bn = nn.BatchNorm2d(cout)

    def forward(self, x):
        x = self.conv(x)
        h = x
        x = self.conv1(x)
        x = self.bn(x)
        x = h + x
        x = self.relu(x)
        return x

class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction_ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        
        self.fc = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        out = avg_out + max_out
        return self.sigmoid(out) * x

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x_cat = torch.cat([avg_out, max_out], dim=1)
        x_out = self.conv(x_cat)
        return self.sigmoid(x_out) * x

class CBAM(nn.Module):
    def __init__(self, in_channels, reduction_ratio=16, kernel_size=7):
        super(CBAM, self).__init__()
        self.channel_att = ChannelAttention(in_channels, reduction_ratio)
        self.spatial_att = SpatialAttention(kernel_size)
    
    def forward(self, x):
        x = self.channel_att(x)
        x = self.spatial_att(x)
        return x

class CustomEncoder(nn.Module):
    def __init__(self, encoder_name, features_only=True):
        super(CustomEncoder, self).__init__()
        # Enhanced encoder options - removed EfficientNet and added alternatives
        if encoder_name == "resnet101":
            self.encoder = models.resnet101(pretrained=True)
            self.channels = [64, 256, 512, 1024, 2048]
        elif encoder_name == "resnet50":
            self.encoder = models.resnet50(pretrained=True)
            self.channels = [64, 256, 512, 1024, 2048]
        elif encoder_name == "resnet34":
            self.encoder = models.resnet34(pretrained=True)
            self.channels = [64, 64, 128, 256, 512]
        elif encoder_name == "densenet121":
            self.encoder = models.densenet121(pretrained=True)
            self.channels = [64, 128, 256, 512, 1024]
            self.densenet = True
        elif encoder_name == "mobilenet_v3":
            self.encoder = models.mobilenet_v3_large(pretrained=True)
            self.channels = [16, 24, 40, 112, 960]
            self.mobilenet = True
        elif encoder_name == "swin_transformer":
            # Using torchvision's Swin-T as pretrained base
            self.encoder = models.swin_t(pretrained=True)
            # Swin Transformer feature channels
            self.channels = [96, 192, 384, 768, 768]
            self.swin = True
        else:
            raise ValueError(f"Unsupported encoder: {encoder_name}")
        
        self.encoder_name = encoder_name
        self.swin = 'swin' in encoder_name
        self.densenet = 'densenet' in encoder_name
        self.mobilenet = 'mobilenet' in encoder_name
        
        # Create adapter layers to standardize outputs to larger dimensions
        # [64, 128, 256, 512, 1024] - increased channel dimensions for better representation
        self.adapters = nn.ModuleList([
            ConvBNReLU(self.channels[0], 64, 1, padding=0),
            ConvBNReLU(self.channels[1], 128, 1, padding=0),
            ConvBNReLU(self.channels[2], 256, 1, padding=0),
            ConvBNReLU(self.channels[3], 512, 1, padding=0),
            ConvBNReLU(self.channels[4], 1024, 1, padding=0)
        ])
        
        # Add channel attention to each adapter for feature refinement
        self.attention = nn.ModuleList([
            ChannelAttention(64, reduction_ratio=8),
            ChannelAttention(128, reduction_ratio=8),
            ChannelAttention(256, reduction_ratio=8),
            ChannelAttention(512, reduction_ratio=8),
            ChannelAttention(1024, reduction_ratio=8)
        ])

    def forward(self, x):
        if self.swin:
            # Swin Transformer forward pass
            features = []
            # Extract hierarchical features
            patch_embed = self.encoder.features[0][0]  # Patch embedding
            norm_layer = self.encoder.features[0][1]   # Norm layer
            
            x = patch_embed(x)
            x = norm_layer(x)
            features.append(self.attention[0](self.adapters[0](x)))  # 96 -> 64
            
            x = self.encoder.features[1](x)  # Stage 1
            features.append(self.attention[1](self.adapters[1](x)))  # 192 -> 128
            
            x = self.encoder.features[2](x)  # Stage 2
            features.append(self.attention[2](self.adapters[2](x)))  # 384 -> 256
            
            x = self.encoder.features[3](x)  # Stage 3
            features.append(self.attention[3](self.adapters[3](x)))  # 768 -> 512
            
            x = self.encoder.features[4](x)  # Final norm layer
            features.append(self.attention[4](self.adapters[4](x)))  # 768 -> 1024
            
            return features
            
        elif self.densenet:
    # DenseNet forward pass
            features = []
            
            # First conv
            x = self.encoder.features.conv0(x)
            x = self.encoder.features.norm0(x)
            x = self.encoder.features.relu0(x)
            features.append(self.attention[0](self.adapters[0](x)))  # 64 -> 64
            
            # Max pooling
            x = self.encoder.features.pool0(x)
            
            # Dense blocks
            x = self.encoder.features.denseblock1(x)
            x = self.encoder.features.transition1(x)
            # Add debugging print statements here
            # print(f"After transition1, x shape: {x.shape}")  # Check actual channel count
            # print(f"Expected channels in adapter: {self.channels[1]}")
            features.append(self.attention[1](self.adapters[1](x)))  # 256 -> 128
            
            x = self.encoder.features.denseblock2(x)
            x = self.encoder.features.transition2(x)
            # You can add similar debugging prints here too
            # print(f"After transition2, x shape: {x.shape}")
            # print(f"Expected channels in adapter: {self.channels[2]}")
            features.append(self.attention[2](self.adapters[2](x)))  # 512 -> 256
            
            x = self.encoder.features.denseblock3(x)
            x = self.encoder.features.transition3(x)
            # And here
            # print(f"After transition3, x shape: {x.shape}")
            # print(f"Expected channels in adapter: {self.channels[3]}")
            features.append(self.attention[3](self.adapters[3](x)))  # 1024 -> 512
            
            x = self.encoder.features.denseblock4(x)
            x = self.encoder.features.norm5(x)
            # And here
            # print(f"After denseblock4, x shape: {x.shape}")
            # print(f"Expected channels in adapter: {self.channels[4]}")
            features.append(self.attention[4](self.adapters[4](x)))  # 1024 -> 1024 # 1024 -> 1024
            
            return features
            
        elif self.mobilenet:
            # MobileNetV3 forward pass
            features = []
            
            # Create intermediate feature extraction points
            layers = list(self.encoder.features)
            
            # First layer
            x = layers[0](x)  # First conv layer
            features.append(self.attention[0](self.adapters[0](x)))  # 16 -> 64
            
            # Second set of layers
            x = nn.Sequential(*layers[1:4])(x)
            features.append(self.attention[1](self.adapters[1](x)))  # 24 -> 128
            
            # Third set of layers
            x = nn.Sequential(*layers[4:7])(x)
            features.append(self.attention[2](self.adapters[2](x)))  # 40 -> 256
            
            # Fourth set of layers
            x = nn.Sequential(*layers[7:13])(x)
            features.append(self.attention[3](self.adapters[3](x)))  # 112 -> 512
            
            # Final set of layers
            x = nn.Sequential(*layers[13:])(x)
            features.append(self.attention[4](self.adapters[4](x)))  # 960 -> 1024
            
            return features
            
        else:  # ResNet variants
            # ResNet forward pass
            features = []
            x = self.encoder.conv1(x)
            x = self.encoder.bn1(x)
            x = self.encoder.relu(x)
            features.append(self.attention[0](self.adapters[0](x)))  # 64 -> 64
            
            x = self.encoder.maxpool(x)
            x = self.encoder.layer1(x)
            features.append(self.attention[1](self.adapters[1](x)))  # varies -> 128
            
            x = self.encoder.layer2(x)
            features.append(self.attention[2](self.adapters[2](x)))  # varies -> 256
            
            x = self.encoder.layer3(x)
            features.append(self.attention[3](self.adapters[3](x)))  # varies -> 512
            
            x = self.encoder.layer4(x)
            features.append(self.attention[4](self.adapters[4](x)))  # varies -> 1024
            
            return features

# Advanced Feature Fusion Module with Attention
class AttentionFusionModule(nn.Module):
    def __init__(self, in_channels):
        super(AttentionFusionModule, self).__init__()
        # Reduce channel dimension for efficiency
        self.reduce_dim = in_channels // 2
        
        # Weight generators for each input feature
        self.weight_gen1 = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channels, 1, kernel_size=1),
            nn.Sigmoid()
        )
        self.weight_gen2 = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channels, 1, kernel_size=1),
            nn.Sigmoid()
        )
        self.weight_gen3 = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channels, 1, kernel_size=1),
            nn.Sigmoid()
        )
        
        # Projection layers
        self.proj1 = ConvBNReLU(in_channels, self.reduce_dim, 1, padding=0)
        self.proj2 = ConvBNReLU(in_channels, self.reduce_dim, 1, padding=0)
        self.proj3 = ConvBNReLU(in_channels, self.reduce_dim, 1, padding=0)
        
        # Final fusion
        self.fuse = ConvBNReLU(self.reduce_dim * 3, in_channels, 3, padding=1)
        self.final_attention = CBAM(in_channels, reduction_ratio=8)
        
    def forward(self, *features):
        # Make sure all features have the same size
        sizes = [f.shape[2:] for f in features]
        target_size = min(sizes, key=lambda s: s[0] * s[1])
        
        resized_features = []
        for i, f in enumerate(features):
            if f.shape[2:] != target_size:
                f = nn.functional.interpolate(f, size=target_size, mode='bilinear', align_corners=True)
            resized_features.append(f)
        
        # Apply attention weights
        f1 = self.proj1(resized_features[0]) * self.weight_gen1(resized_features[0])
        f2 = self.proj2(resized_features[1]) * self.weight_gen2(resized_features[1])
        f3 = self.proj3(resized_features[2]) * self.weight_gen3(resized_features[2])
        
        # Concatenate and fuse
        concat_features = torch.cat([f1, f2, f3], dim=1)
        fused = self.fuse(concat_features)
        fused = self.final_attention(fused)
        
        return fused

# Pyramid Pooling Module for capturing multi-scale context
class PyramidPoolingModule(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(PyramidPoolingModule, self).__init__()
        self.pools = nn.ModuleList([
            nn.Sequential(
                nn.AdaptiveAvgPool2d(bin_size),
                ConvBNReLU(in_channels, out_channels // 4, 1, padding=0)
            ) for bin_size in [(1, 1), (2, 2), (3, 3), (6, 6)]
        ])
        self.bottleneck = ConvBNReLU(in_channels + out_channels, out_channels, 3, padding=1)
        
    def forward(self, x):
        h, w = x.size(2), x.size(3)
        features = [x]
        
        for pool in self.pools:
            feat = pool(x)
            feat = F.interpolate(feat, size=(h, w), mode='bilinear', align_corners=True)
            features.append(feat)
            
        fused = torch.cat(features, dim=1)
        return self.bottleneck(fused)

# Enhanced Decoder Block with Residual Connections
class EnhancedDecoderBlock(nn.Module):
    def __init__(self, in_channels, skip_channels, out_channels):
        super(EnhancedDecoderBlock, self).__init__()
        self.upsample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)
        
        self.conv_block = nn.Sequential(
            ConvBNReLU(out_channels + skip_channels, out_channels, 3, padding=1),
            ConvBNReLU(out_channels, out_channels, 3, padding=1, activation=False)
        )
        
        self.relu = nn.ReLU()
        self.identity_map = nn.Conv2d(out_channels + skip_channels, out_channels, 1)
        self.norm = nn.BatchNorm2d(out_channels)
        self.attention = CBAM(out_channels)
        
    def forward(self, x, skip):
        x = self.upsample(x)
        
        # Resize if dimensions don't match
        if x.shape[2:] != skip.shape[2:]:
            x = F.interpolate(x, skip.shape[2:], mode='bilinear', align_corners=True)
        
        # Concatenate skip connection
        concat = torch.cat([x, skip], dim=1)
        
        # Residual connection
        identity = self.identity_map(concat)
        
        # Main path
        out = self.conv_block(concat)
        out = out + identity
        out = self.relu(out)
        out = self.norm(out)
        
        # Apply attention
        out = self.attention(out)
        
        return out

# Improved MTUNet with alternative encoders (removed EfficientNet)
class MTUNet(nn.Module):
    def __init__(self, out_ch=4, in_channels=3):
        super(MTUNet, self).__init__()
        
        # Enhanced encoders - replaced EfficientNet with ResNet50 and DenseNet121
        self.encoder1 = CustomEncoder("resnet101")
        self.encoder2 = CustomEncoder("resnet50")  # Changed from EfficientNet to ResNet50
        self.encoder3 = CustomEncoder("densenet121")  # Alternative to Swin Transformer
        
        # Feature dimensions
        self.feature_dims = [64, 128, 256, 512, 1024]
        
        # Advanced fusion modules with attention mechanisms
        self.fusion_modules = nn.ModuleList([
            AttentionFusionModule(self.feature_dims[0]),  # Scale 1 (1/2)
            AttentionFusionModule(self.feature_dims[1]),  # Scale 2 (1/4)
            AttentionFusionModule(self.feature_dims[2]),  # Scale 3 (1/8)
            AttentionFusionModule(self.feature_dims[3]),  # Scale 4 (1/16)
            AttentionFusionModule(self.feature_dims[4])   # Scale 5 (1/32)
        ])
        
        # Add a pyramid pooling module at the bottleneck
        self.ppm = PyramidPoolingModule(self.feature_dims[4], self.feature_dims[4])
        
        # Enhanced decoder blocks with advanced skip connections
        self.decoder_blocks = nn.ModuleList([
            # From lowest resolution (1/32) to (1/16)
            EnhancedDecoderBlock(self.feature_dims[4], self.feature_dims[3], self.feature_dims[3]),
            # From (1/16) to (1/8)
            EnhancedDecoderBlock(self.feature_dims[3], self.feature_dims[2], self.feature_dims[2]),
            # From (1/8) to (1/4)
            EnhancedDecoderBlock(self.feature_dims[2], self.feature_dims[1], self.feature_dims[1]),
            # From (1/4) to (1/2)
            EnhancedDecoderBlock(self.feature_dims[1], self.feature_dims[0], self.feature_dims[0])
        ])
        
        # Final convolutional layers for fine detail preservation
        self.detail_recovery = nn.Sequential(
            ConvBNReLU(self.feature_dims[0], self.feature_dims[0] // 2, 3, padding=1),
            ConvBNReLU(self.feature_dims[0] // 2, self.feature_dims[0] // 2, 3, padding=1),
            nn.Conv2d(self.feature_dims[0] // 2, out_ch, kernel_size=1)
        )
        
        self.in_channels = in_channels
    
    def forward(self, x):
        # Handle single-channel input (for grayscale images like in ACDC dataset)
        if x.size(1) == 1 and self.in_channels == 3:
            x = x.repeat(1, 3, 1, 1)
        
        # Extract features from each encoder
        features1 = self.encoder1(x)
        features2 = self.encoder2(x)
        features3 = self.encoder3(x)
        
        # Fuse features from all encoders at each scale using advanced fusion
        fused_features = []
        for i in range(5):  # 5 scales
            fused = self.fusion_modules[i](features1[i], features2[i], features3[i])
            fused_features.append(fused)
        
        # Apply PPM at the bottleneck for multi-scale context
        x = self.ppm(fused_features[4])
        
        # Decoder path with enhanced skip connections
        for i, decoder_block in enumerate(self.decoder_blocks):
            skip_features = fused_features[3 - i]  # Use skip connections in reverse order
            x = decoder_block(x, skip_features)
        
        # Final upsampling to original input size if needed
        if x.shape[2:] != x.shape[2] * 2:
            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)
        
        # Final convolution with detail recovery
        x = self.detail_recovery(x)
        
        return x

# Alternative version with MobileNet and ResNet combination (lighter model)
class LightMTUNet(nn.Module):
    def __init__(self, out_ch=4, in_channels=3):
        super(LightMTUNet, self).__init__()
        
        # Lighter encoder combination
        self.encoder1 = CustomEncoder("mobilenet_v3")
        self.encoder2 = CustomEncoder("resnet34")
        self.encoder3 = CustomEncoder("resnet50")
        
        # Feature dimensions
        self.feature_dims = [64, 128, 256, 512, 1024]
        
        # Advanced fusion modules with attention mechanisms
        self.fusion_modules = nn.ModuleList([
            AttentionFusionModule(self.feature_dims[0]),
            AttentionFusionModule(self.feature_dims[1]),
            AttentionFusionModule(self.feature_dims[2]),
            AttentionFusionModule(self.feature_dims[3]),
            AttentionFusionModule(self.feature_dims[4])
        ])
        
        # Add a pyramid pooling module at the bottleneck
        self.ppm = PyramidPoolingModule(self.feature_dims[4], self.feature_dims[4])
        
        # Enhanced decoder blocks with advanced skip connections
        self.decoder_blocks = nn.ModuleList([
            EnhancedDecoderBlock(self.feature_dims[4], self.feature_dims[3], self.feature_dims[3]),
            EnhancedDecoderBlock(self.feature_dims[3], self.feature_dims[2], self.feature_dims[2]),
            EnhancedDecoderBlock(self.feature_dims[2], self.feature_dims[1], self.feature_dims[1]),
            EnhancedDecoderBlock(self.feature_dims[1], self.feature_dims[0], self.feature_dims[0])
        ])
        
        # Final convolutional layers for fine detail preservation
        self.detail_recovery = nn.Sequential(
            ConvBNReLU(self.feature_dims[0], self.feature_dims[0] // 2, 3, padding=1),
            ConvBNReLU(self.feature_dims[0] // 2, self.feature_dims[0] // 2, 3, padding=1),
            nn.Conv2d(self.feature_dims[0] // 2, out_ch, kernel_size=1)
        )
        
        self.in_channels = in_channels
    
    def forward(self, x):
        # Handle single-channel input
        if x.size(1) == 1 and self.in_channels == 3:
            x = x.repeat(1, 3, 1, 1)
        
        # Extract features from each encoder
        features1 = self.encoder1(x)
        features2 = self.encoder2(x)
        features3 = self.encoder3(x)
        
        # Fuse features from all encoders at each scale
        fused_features = []
        for i in range(5):
            fused = self.fusion_modules[i](features1[i], features2[i], features3[i])
            fused_features.append(fused)
        
        # Apply PPM at the bottleneck
        x = self.ppm(fused_features[4])
        
        # Decoder path
        for i, decoder_block in enumerate(self.decoder_blocks):
            skip_features = fused_features[3 - i]
            x = decoder_block(x, skip_features)
        
        # Final upsampling if needed
        if x.shape[2:] != x.shape[2] * 2:
            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)
        
        # Final convolution
        x = self.detail_recovery(x)
        
        return x

# Example usage
if __name__ == "__main__":
    model = MTUNet(out_ch=4)
    x = torch.randn(2, 3, 224, 224)
    y = model(x)
    print(y.shape)  # Should be [2, 4, 224, 224]
    
    # Test the lighter model
    light_model = LightMTUNet(out_ch=4)
    y_light = light_model(x)
    print(y_light.shape)  # Should be [2, 4, 224, 224]
